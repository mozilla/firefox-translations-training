import pytest

from pipeline.alignments.tokenizer import IcuTokenizer, TokenizerType, tokenize
from fixtures import zh_sample, en_sample, ru_sample, DataDir

tokenized_first_lines = {
    "en": "The â– little â– girl , â– seeing â– she â– had â– lost â– one â– of â– her â– pretty â– shoes , â– grew â– angry , â– and â– said â– to â– the â– Witch , â– â€œ Give â– me â– back â– my â– shoe ! â€",
    "ru": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ â– Ğ´ĞµĞ²Ğ¾Ñ‡ĞºĞ° , â– ÑƒĞ²Ğ¸Ğ´ĞµĞ² , â– Ñ‡Ñ‚Ğ¾ â– Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ»Ğ° â– Ğ¾Ğ´Ğ½Ñƒ â– Ğ¸Ğ· â– ÑĞ²Ğ¾Ğ¸Ñ… â– ĞºÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ñ… â– Ñ‚ÑƒÑ„ĞµĞ»ĞµĞº , â– Ñ€Ğ°ÑÑĞµÑ€Ğ´Ğ¸Ğ»Ğ°ÑÑŒ â– Ğ¸ â– ÑĞºĞ°Ğ·Ğ°Ğ»Ğ° â– Ğ’ĞµĞ´ÑŒĞ¼Ğµ : â– Â« Ğ’ĞµÑ€Ğ½Ğ¸ â– Ğ¼Ğ½Ğµ â– Ğ¼Ğ¾Ñ â– Ñ‚ÑƒÑ„ĞµĞ»ÑŒĞºÑƒ ! Â»",
    "zh": "å° å¥³å­© çœ‹åˆ° è‡ªå·± ä¸¢ äº† ä¸€åª æ¼‚äº® çš„ é‹å­ ï¼Œ ç”Ÿæ°” äº† ï¼Œ å¯¹ å¥³å·« è¯´ ï¼š â€œ æŠŠ æˆ‘çš„ é‹å­ è¿˜ç»™ æˆ‘ ï¼ â€",
}


@pytest.mark.parametrize(
    "lang,sample,first_line",
    [
        ("en", en_sample, tokenized_first_lines["en"]),
        ("ru", ru_sample, tokenized_first_lines["ru"]),
        ("zh", zh_sample, tokenized_first_lines["zh"]),
        ("zh", "è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯­å¥ ğŸ¤£ ã€‚", "è¿™ æ˜¯ ä¸€ä¸ª ç®€å• çš„ æµ‹è¯• è¯­ å¥ â– ğŸ¤£â– ã€‚"),
    ],
    ids=["en", "ru", "zh", "zh2"],
)
def test_icu_tokenize_detokenize(lang, sample, first_line):
    lines = sample.splitlines()
    tokenizer = IcuTokenizer
    icu_tokenizer = tokenizer(lang)
    tok_lines = []
    detok_lines = []

    for line in lines:
        tokens = icu_tokenizer.tokenize(line)
        detokenized = icu_tokenizer.detokenize(tokens)
        tok_lines.append(" ".join(tokens))
        detok_lines.append(detokenized)

    assert lines == detok_lines
    assert tok_lines[0] == first_line


@pytest.mark.parametrize(
    "lang,sample",
    [
        (
            "en",
            en_sample,
        ),
        (
            "ru",
            ru_sample,
        ),
        ("zh", zh_sample),
    ],
    ids=["en", "ru", "zh"],
)
def test_tokenizer(lang, sample):
    data_dir = DataDir("test_tokenizer")
    input_path = data_dir.create_file(f"input.{lang}.txt", sample)
    output_path = data_dir.join(f"output.{lang}.txt")

    tokenize(
        input_path=input_path,
        output_path=output_path,
        lang=lang,
        tokenizer=TokenizerType.icu,
        sentences_per_chunk=3,
    )

    with open(output_path) as f:
        lines = f.read().splitlines()

    assert len(lines) == len(sample.splitlines())
    assert lines[0] == tokenized_first_lines[lang]
