import pytest

from pipeline.alignments.tokenizer import IcuTokenizer, TokenizerType, tokenize
from fixtures import zh_sample, en_sample, ru_sample, DataDir

tokenized_first_lines = {
    "en": "The â– little â– girl , â– seeing â– she â– had â– lost â– one â– of â– her â– pretty â– shoes , â– grew â– angry , â– and â– said â– to â– the â– Witch , â– â€œ Give â– me â– back â– my â– shoe ! â€",
    "ru": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ â– Ğ´ĞµĞ²Ğ¾Ñ‡ĞºĞ° , â– ÑƒĞ²Ğ¸Ğ´ĞµĞ² , â– Ñ‡Ñ‚Ğ¾ â– Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ»Ğ° â– Ğ¾Ğ´Ğ½Ñƒ â– Ğ¸Ğ· â– ÑĞ²Ğ¾Ğ¸Ñ… â– ĞºÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ñ… â– Ñ‚ÑƒÑ„ĞµĞ»ĞµĞº , â– Ñ€Ğ°ÑÑĞµÑ€Ğ´Ğ¸Ğ»Ğ°ÑÑŒ â– Ğ¸ â– ÑĞºĞ°Ğ·Ğ°Ğ»Ğ° â– Ğ’ĞµĞ´ÑŒĞ¼Ğµ : â– Â« Ğ’ĞµÑ€Ğ½Ğ¸ â– Ğ¼Ğ½Ğµ â– Ğ¼Ğ¾Ñ â– Ñ‚ÑƒÑ„ĞµĞ»ÑŒĞºÑƒ ! Â»",
    "zh": "å° å¥³å­© çœ‹åˆ° è‡ªå·± ä¸¢ äº† ä¸€åª æ¼‚äº® çš„ é‹å­ ï¼Œ ç”Ÿæ°” äº† ï¼Œ å¯¹ å¥³å·« è¯´ ï¼š â€œ æŠŠ æˆ‘çš„ é‹å­ è¿˜ç»™ æˆ‘ ï¼ â€",
}


@pytest.mark.parametrize(
    "lang,sample,first_line",
    [
        ("en", en_sample, tokenized_first_lines["en"]),
        ("ru", ru_sample, tokenized_first_lines["ru"]),
        ("zh", zh_sample, tokenized_first_lines["zh"]),
        ("zh", "è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯­å¥ ğŸ¤£ ã€‚", "è¿™ æ˜¯ ä¸€ä¸ª ç®€å• çš„ æµ‹è¯• è¯­ å¥ â– ğŸ¤£â– ã€‚"),
    ],
    ids=["en", "ru", "zh", "zh2"],
)
def test_icu_tokenize_detokenize(lang, sample, first_line):
    lines = sample.splitlines()
    tokenizer = IcuTokenizer
    icu_tokenizer = tokenizer(lang)
    tok_lines = []
    detok_lines = []

    for line in lines:
        tokens = icu_tokenizer.tokenize(line)
        detokenized = icu_tokenizer.detokenize(tokens)
        tok_lines.append(" ".join(tokens))
        detok_lines.append(detokenized)

    assert lines == detok_lines
    assert tok_lines[0] == first_line


@pytest.mark.parametrize(
    "lang,sample",
    [
        (
            "en",
            en_sample,
        ),
        (
            "ru",
            ru_sample,
        ),
        ("zh", zh_sample),
    ],
    ids=["en", "ru", "zh"],
)
def test_tokenizer(lang, sample):
    data_dir = DataDir("test_tokenizer")
    input_path = data_dir.create_file(f"input.{lang}.txt", sample)
    output_path = data_dir.join(f"output.{lang}.txt")

    tokenize(
        input_path=input_path,
        output_path=output_path,
        lang=lang,
        tokenizer=TokenizerType.icu,
        sentences_per_chunk=3,
    )

    with open(output_path) as f:
        lines = f.read().splitlines()

    assert len(lines) == len(sample.splitlines())
    assert lines[0] == tokenized_first_lines[lang]


@pytest.mark.parametrize(
    "lang,text,expected_tokenized",
    [
        (
            "en",
            "This, is a sentence with weird\xbb symbols\u2026 appearing everywhere\xbf",
            "This , â– is â– a â– sentence â– with â– weird Â» â– symbols â€¦ â– appearing â– everywhere Â¿",
        ),
        ("en", "abc def.", "abc â– def ."),
        ("en", "2016, pp.", "2016 , â– pp ."),
        (
            "en",
            "This ain't funny. It's actually hillarious, yet double Ls. | [] < > [ ] & You're gonna shake it off? Don't?",
            "This â– ain't â– funny . â– It's â– actually â– hillarious , â– yet â– double â– Ls . â– | â– [ ] â– < â– > â– [ â– ] â– & â– You're â– gonna â– shake â– it â– off ? â– Don't ?",
        ),
        ("en", "this 'is' the thing", "this â– ' is ' â– the â– thing"),
        (
            "en",
            "By the mid 1990s a version of the game became a Latvian television series (with a parliamentary setting, and played by Latvian celebrities).",
            "By â– the â– mid â– 1990s â– a â– version â– of â– the â– game â– became â– a â– Latvian â– television â– series â– ( with â– a â– parliamentary â– setting , â– and â– played â– by â– Latvian â– celebrities ) .",
        ),
        (
            "en",
            "The meeting will take place at 11:00 a.m. Tuesday.",
            "The â– meeting â– will â– take â– place â– at â– 11 : 00 â– a.m . â– Tuesday .",
        ),
        ("en", "'Hello.'", "' Hello . '"),
        ("en", "'So am I.", "' So â– am â– I ."),
        (
            "fr",
            "Des gens admirent une Å“uvre d'art.",
            "Des â– gens â– admirent â– une â– Å“uvre â– d'art .",
        ),
        ("de", "...schwer wie ein iPhone 5.", ". . . schwer â– wie â– ein â– iPhone â– 5 ."),
        ("cz", "DvÄ› dÄ›ti, kterÃ© bÄ›Å¾Ã­ bez bot.", "DvÄ› â– dÄ›ti , â– kterÃ© â– bÄ›Å¾Ã­ â– bez â– bot ."),
        (
            "en",
            "this is a webpage https://stackoverflow.com/questions/6181381/how-to-print-variables-in-perl that kicks ass",
            "this â– is â– a â– webpage â– https : / / stackoverflow.com / questions / 6181381 / how - to - print - variables - in - perl â– that â– kicks â– ass",
        ),
        (
            "en",
            "What about a this,type,of-s-thingy?",
            "What â– about â– a â– this , type , of - s - thingy ?",
        ),
        (
            "de",
            "Sie sollten vor dem Upgrade eine Sicherung dieser Daten erstellen (wie unter Abschnitt 4.1.1, â€Sichern aller Daten und Konfigurationsinformationenâ€œ beschrieben). ",
            "Sie â– sollten â– vor â– dem â– Upgrade â– eine â– Sicherung â– dieser â– Daten â– erstellen â– ( wie â– unter â– Abschnitt â– 4.1.1 , â– â€ Sichern â– aller â– Daten â– und â– Konfigurationsinformationen â€œ â– beschrieben ) . â–",
        ),
        (
            "fr",
            "L'amitiÃ© nous a fait forts d'esprit",
            "L'amitiÃ© â– nous â– a â– fait â– forts â– d'esprit",
        ),
        ("zh", "è®°è€… åº”è°¦ ç¾å›½", "è®°è€… â– åº” è°¦ â– ç¾å›½"),
        ("ko", "ì„¸ê³„ ì—ì„œ ê°€ì¥ ê°•ë ¥í•œ.", "ì„¸ê³„ â– ì—ì„œ â– ê°€ì¥ â– ê°•ë ¥í•œ ."),
        ("ja", "é›»è©±ã§ã‚“ã‚ã®é‚ªé­”ã˜ã‚ƒã¾ã‚’ã—ãªã„ã§ãã ã•ã„", "é›»è©± ã§ã‚“ã‚ ã® é‚ªé­” ã˜ã‚ƒ ã¾ ã‚’ ã—ãªã„ ã§ ãã  ã•ã„"),
        ("ja", "Japan is æ—¥æœ¬ in Japanese.", "Japan â– is â– æ—¥æœ¬ â– in â– Japanese ."),
    ],
    ids=[
        "en_weird_symbols",
        "en_fullstop",
        "en_numeric_prefix",
        "en_braces",
        "en_apostrophe",
        "en_opening_brackets",
        "en_dot_splitting",
        "en_trailing_dot_apostrophe",
        "en_one_apostrophe",
        "fr",
        "de",
        "cz",
        "en_pattern1",
        "en_pattern2",
        "de_final_comma_split_after_number",
        "fr_apostrophes",
        "zh",
        "ko",
        "ja",
        "cjk_mix",
    ],
)
def test_icu_tokens(lang, text, expected_tokenized):
    """
    Tests tokens produced by ICU tokenizer.

    The use cases were copied from https://github.com/hplt-project/sacremoses/blob/master/sacremoses/test/test_tokenizer.py as is.
    However, this test is mostly to show how the tokenizer works rather than fixing it because it relies on the underlying ICU tokenizer.
    The expected values were just copied from the test run.
    Having some mistakes in tokenization is ok because it's used only for the purposes of inline noise augmentation and to produce word alignments.
    """
    icu_tokenizer = IcuTokenizer(lang)

    tokens = icu_tokenizer.tokenize(text)
    tokenized = " ".join(tokens)

    assert expected_tokenized == tokenized
