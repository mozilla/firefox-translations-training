# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#
# This kind primarily exists because these dataset fetches break
# some assumptions made the `job` transforms that treat the `fetch`
# kind specially.
---
loader: taskgraph.loader.transform:loader

transforms:
    - translations_taskgraph.transforms.from_datasets:per_dataset
    - translations_taskgraph.transforms.worker_selection
    - taskgraph.transforms.task_context
    - taskgraph.transforms.run:transforms
    - translations_taskgraph.transforms.cached_tasks:transforms
    - taskgraph.transforms.task:transforms

task-defaults:
    worker-type: b-cpu
    attributes:
        cache:
            type: dataset
    dataset-config:
        substitution-fields:
            - name
            - label
            - worker.env
            - run.command
    task-context:
        substitution-fields: []
    worker:
        chain-of-trust: true
        docker-image: {in-tree: toolchain-build}
        max-run-time: 86400
        env:
            COMPRESSION_CMD: zstdmt
            ARTIFACT_EXT: zst
            SRC: "{src_locale}"
            TRG: "{trg_locale}"
        artifacts:
            - name: public/build
              path: /builds/worker/artifacts
              type: directory
        # 128 happens when cloning this repository fails
        retry-exit-status: [128]

    run-on-tasks-for: []
    run:
        using: run-task

tasks:
    flores:
        description: Fetch flores101 dataset
        label: dataset-flores-{dataset_sanitized}-{src_locale}-{trg_locale}
        dataset-config:
            provider: flores
        attributes:
            cache:
                resources:
                    - pipeline/data/importers/corpus/flores.sh
                    - pipeline/data/download-corpus.sh
                    - pipeline/data/dataset_importer.py
                    - pipeline/data/requirements/data.txt
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/dataset_importer.py
                    --type corpus
                    --dataset {dataset}
                    --output_prefix $TASK_WORKDIR/artifacts/{dataset_sanitized}

    sacrebleu:
        description: Fetch sacrebleu dataset
        label: dataset-sacrebleu-{dataset_sanitized}-{src_locale}-{trg_locale}
        dataset-config:
            provider: sacrebleu
        attributes:
            cache:
                resources:
                    - pipeline/data/importers/corpus/sacrebleu.sh
                    - pipeline/data/download-corpus.sh
                    - pipeline/data/dataset_importer.py
                    - pipeline/data/requirements/data.txt
        run:
            command:
                - bash
                - -cx
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    python3 -u $VCS_PATH/pipeline/data/dataset_importer.py
                    --type corpus
                    --dataset {dataset}
                    --output_prefix $TASK_WORKDIR/artifacts/{dataset_sanitized} 2>&1

    opus:
        description: Fetch opus dataset
        # No slashes version of dataset used here because slashes break caches
        label: dataset-opus-{dataset_sanitized}-{src_locale}-{trg_locale}
        dataset-config:
            provider: opus
        attributes:
            cache:
                resources:
                    - pipeline/data/importers/corpus/opus.sh
                    - pipeline/data/download-corpus.sh
                    - pipeline/data/dataset_importer.py
                    - pipeline/data/requirements/data.txt
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/dataset_importer.py
                    --type corpus
                    --dataset {dataset}
                    --output_prefix $TASK_WORKDIR/artifacts/{dataset_sanitized}

    url:
        description: Fetch a parallel corpus from a URL
        # No slashes version of dataset used here because slashes break caches
        label: dataset-url-{dataset_sanitized}-{src_locale}-{trg_locale}
        dataset-config:
            provider: url
        attributes:
            cache:
                resources:
                    - pipeline/data/importers/corpus/url.py
                    - pipeline/data/download-corpus.sh
                    - pipeline/data/dataset_importer.py
                    - pipeline/data/requirements/data.txt
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/dataset_importer.py
                    --type corpus
                    --dataset {dataset}
                    --output_prefix $TASK_WORKDIR/artifacts/{dataset_sanitized}

    url-{src_locale}:
        description: Fetch a monolingual corpus from a URL for {src_locale}
        label: dataset-url-{dataset_sanitized}-{src_locale}
        dataset-config:
            provider: url
            category: mono-src
        attributes:
            cache:
                resources:
                    - pipeline/data/download-mono.py
                from-parameters:
                    max_sentences:
                        - training_config.experiment.mono-max-sentences-src
        task-context:
            from-parameters:
                max_sentences:
                    - training_config.experiment.mono-max-sentences-src
            substitution-fields:
                - run.command
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/download-mono.py
                    --dataset {dataset}
                    --language {src_locale}
                    --max_sentences {max_sentences}
                    --artifacts $TASK_WORKDIR/artifacts

    url-{trg_locale}:
        description: Fetch a monolingual corpus from a URL for {trg_locale}
        label: dataset-url-{dataset_sanitized}-{trg_locale}
        dataset-config:
            provider: url
            category: mono-trg
        attributes:
            cache:
                resources:
                    - pipeline/data/download-mono.py
                from-parameters:
                    max_sentences:
                        - training_config.experiment.mono-max-sentences-trg
        task-context:
            from-parameters:
                max_sentences:
                    - training_config.experiment.mono-max-sentences-trg
            substitution-fields:
                - run.command
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/download-mono.py
                    --dataset {dataset}
                    --language {trg_locale}
                    --max_sentences {max_sentences}
                    --artifacts $TASK_WORKDIR/artifacts

    mtdata:
        description: Fetch mtdata dataset
        label: dataset-mtdata-{dataset_sanitized}-{src_locale}-{trg_locale}
        dataset-config:
            provider: mtdata
        attributes:
            cache:
                resources:
                    - pipeline/data/importers/corpus/mtdata.sh
                    - pipeline/data/download-corpus.sh
                    - pipeline/data/dataset_importer.py
                    - pipeline/data/requirements/data.txt
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/dataset_importer.py
                    --type corpus
                    --dataset {dataset}
                    --output_prefix $TASK_WORKDIR/artifacts/{dataset_sanitized}

    news-crawl-{src_locale}:
        description: Fetch news-crawl dataset for {src_locale}
        label: dataset-news-crawl-{dataset_sanitized}-{src_locale}
        dataset-config:
            provider: news-crawl
            category: mono-src
        attributes:
            cache:
                resources:
                    - pipeline/data/download-mono.py
                from-parameters:
                    max_sentences:
                        - training_config.experiment.mono-max-sentences-src
        task-context:
            from-parameters:
                max_sentences:
                    - training_config.experiment.mono-max-sentences-src
            substitution-fields:
                - run.command
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/download-mono.py
                    --dataset {dataset}
                    --language {src_locale}
                    --max_sentences {max_sentences}
                    --artifacts $TASK_WORKDIR/artifacts

    news-crawl-{trg_locale}:
        description: Fetch news-crawl dataset for {trg_locale}
        label: dataset-news-crawl-{dataset_sanitized}-{trg_locale}
        dataset-config:
            provider: news-crawl
            category: mono-trg
        attributes:
            cache:
                resources:
                    - pipeline/data/download-mono.py
                from-parameters:
                    max_sentences:
                        - training_config.experiment.mono-max-sentences-trg
        task-context:
            from-parameters:
                max_sentences:
                    - training_config.experiment.mono-max-sentences-trg
            substitution-fields:
                - run.command
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/download-mono.py
                    --dataset {dataset}
                    --language {trg_locale}
                    --max_sentences {max_sentences}
                    --artifacts $TASK_WORKDIR/artifacts
