####
# Example of a production config
# Change language pair, experiment name, datasets and other settings if needed
# Training low resource languages might require more tuning of pipeline/training/configs
###


experiment:
  name: prod
  src: ru
  trg: en

  teacher-ensemble: 2
  # path to a pretrained backward model (optional)
  backward-model: ""
  # path to a pretrained vocabulary (optional)
  vocab: ""

  # limits per downloaded dataset
  mono-max-sentences-src: 100000000
  mono-max-sentences-trg: 20000000
  # split corpus to parallelize translation
  split-length: 2000000
  # vocab training sample
  spm-sample-size: 10000000

  best-model: chrf

  bicleaner:
    default-threshold: 0.5
    dataset-thresholds:
      opus_CCAligned/v1: 0.7
      opus_OpenSubtitles/v2018: 0.8
      opus_bible-uedin/v1: 0.7
      mtdata_Statmt-wiki_titles-1-rus-eng: 0.7
      mtdata_Facebook-wikimatrix-1-eng-rus: 0.7
      mtdata_Statmt-wiki_titles-2-rus-eng: 0.7
      mtdata_Statmt-commoncrawl_wmt13-1-rus-eng: 0.7
      # 0 = skip filtering
      opus_ParaCrawl/v8: 0


marian-args:
# these configs override pipeline/train/configs
  training-backward:
    # change based on available training data
    after: 10e
  training-teacher-base:
    # remove for low resource languages or if training without augmentation
    after: 2e
# these configs override pipeline/translate/decoder.yml
  decoding-backward:
    # 12 Gb GPU, s2s model
    mini-batch-words: 2000
    beam-size: 12
  decoding-teacher:
    # 12 Gb GPU, ensemble of 2 teachers
    mini-batch-words: 1000
    # 2080ti or newer
    precision: float16


datasets:
  # parallel training corpus
  train:
    - opus_ada83/v1
    - opus_UN/v20090831
    - opus_GNOME/v1
    - opus_wikimedia/v20210402
    - opus_CCMatrix/v1
    - opus_Wikipedia/v1.0
    - opus_tico-19/v2020-10-28
    - opus_KDE4/v2
    - opus_OpenSubtitles/v2018
    - opus_MultiUN/v1
    - opus_GlobalVoices/v2018q4
    - opus_ELRC_2922/v1
    - opus_PHP/v1
    - opus_Tatoeba/v2021-03-10
    - opus_Tanzil/v1
    - opus_XLEnt/v1.1
    - opus_TildeMODEL/v2018
    - opus_Ubuntu/v14.10
    - opus_TED2013/v1.1
    - opus_infopankki/v1
    - opus_EUbookshop/v2
    - opus_ParaCrawl/v8
    - opus_Books/v1
    - opus_bible-uedin/v1
    - opus_QED/v2.0a
    - opus_CCAligned/v1
    - opus_TED2020/v1
    - opus_News-Commentary/v16
    - opus_UNPC/v1.0
    - mtdata_Statmt-news_commentary-15-eng-rus
    - mtdata_Neulab-tedtalks_train-1-eng-rus
    - mtdata_ELRC-wikipedia_health-1-eng-rus
    - mtdata_ParaCrawl-paracrawl-1_bonus-eng-rus
    - mtdata_Statmt-news_commentary_wmt18-13-rus-eng
    - mtdata_Tilde-worldbank-1-eng-rus
    - mtdata_Statmt-news_commentary-16-eng-rus
    - mtdata_UN-un_test-1-eng-rus
    - mtdata_Statmt-wiki_titles-1-rus-eng
    - mtdata_Statmt-paracrawl-8.wmt21-eng-rus
    - mtdata_LinguaTools-wikititles-2014-eng-rus
    - mtdata_Tilde-airbaltic-1-eng-rus
    - mtdata_Statmt-wiki_titles-2-rus-eng
    - mtdata_Neulab-tedtalks_dev-1-eng-rus
    - mtdata_Statmt-news_commentary-14-eng-rus
    - mtdata_Statmt-commoncrawl_wmt13-1-rus-eng
    - mtdata_Tilde-czechtourism-1-eng-rus
    - mtdata_Facebook-wikimatrix-1-eng-rus
    - mtdata_Neulab-tedtalks_test-1-eng-rus
    - mtdata_Statmt-wikititles-3-rus-eng
  # datasets to merge for validation while training
  devtest:
    - flores_dev
    - sacrebleu_wmt19
    - sacrebleu_wmt17
    - sacrebleu_wmt15
    - sacrebleu_wmt14
  # datasets for evaluation
  test:
    - flores_devtest
    - sacrebleu_wmt20
    - sacrebleu_wmt18
    - sacrebleu_wmt16
    - sacrebleu_wmt13
  # monolingual datasets (ex. paracrawl-mono_paracrawl8, commoncrawl_wmt16, news-crawl_news.2020)
  # to be translated by the teacher model
  mono-src:
    - news-crawl_news.2020
    - news-crawl_news.2019
    - news-crawl_news.2018
    - news-crawl_news.2017
    - news-crawl_news.2016
    - news-crawl_news.2015
    - news-crawl_news.2014
    - news-crawl_news.2013
    - news-crawl_news.2012
    - news-crawl_news.2011
  # to be translated by the backward model to augment teacher corpus with back-translations
  # leave empty to skip augmentation step (high resource languages)
  mono-trg:
    - news-crawl_news.2020
    - news-crawl_news.2019
    - news-crawl_news.2018
    - news-crawl_news.2017
    - news-crawl_news.2016
    - news-crawl_news.2015


