datasets:
  original: <dataset0> # Original parallel corpus
  backtranslated: <dataset1> # Back-translated data

stages:
  - pretrain
  - finetune

# Back-translated corpus can vary a lot in size, so we can try using original one to count epochs
pretrain:
  - original 0.6
  - backtranslated 0.4
  - until original 2

# Fine-tuning only on original clean corpus until the early stopping
finetune:
  - original 1.0
  - until original inf


modifiers:
- UpperCase: 0.07 # Apply randomly to 7% of sentences
- TitleCase: 0.05
- Typos: 0.05
# inserts new noise sentences
- Noise: 0.0005
  min_word_length: 2 # Minimum word length for each word in the noisy sentence
  max_word_length: 5 # Maximum word length for each word in the noisy sentence
  max_words: 6 # Maximum number of words in each noisy sentence
# generates inline noise (emojis etc.) matching positions in source and target sentences using alignments
# no spm_vocab argument -> alignments will be removed from Marian input
# we don't use alignments for teacher training
# Tags modifier has to be the last one to remove the alignments
- Tags: 0.05
  augment: 1


# random seed should be different for different teacher models
seed: <seed>
# parallel sentences + token alignments
num_fields: 3
