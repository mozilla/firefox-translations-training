datasets:
  original: {dataset0} # Original parallel corpus
  backtranslated: {dataset1} # Back-translated data

stages:
  - pretrain
  - finetune

# Train until the model sees two epochs of back-translated corpus
pretrain:
  - original 0.6
  - backtranslated 0.4
  - until backtranslated 2

# Fine-tuning only on original clean corpus until the early stopping
finetune:
  - original 1.0
  - until original inf

# The default values of the modifiers are taken from the paper https://arxiv.org/pdf/2311.14838.pdf
# Please refer to docs/opus-trainer.md for further details
modifiers:
# boost upper case a little as we see that the models underperform on upper case dataset on evaluation
- UpperCase: 0.07 # Apply randomly to 7% of sentences
- TitleCase: 0.05
# Introduce artificial typos in the source text
- Typos: 0.05
# Insert new sentences composed form Unicode noise
- Noise: 0.0005
  min_word_length: 2 # Minimum word length for each word in the noisy sentence
  max_word_length: 5 # Maximum word length for each word in the noisy sentence
  max_words: 6 # Maximum number of words in each noisy sentence
# generates inline noise (emojis etc.) matching positions in source and target sentences using alignments
# no spm_vocab argument -> alignments will be removed from Marian input
# we don't use alignments for teacher training
# Tags modifier has to be the last one to remove the alignments
- Tags: 0.005
  custom_detok_src: "icu:{src}"
  custom_detok_trg: "icu:{trg}"
  augment: 1
  tag: 0


# random seed should be different for different teacher models
seed: {seed}
# parallel sentences + token alignments
num_fields: 3
